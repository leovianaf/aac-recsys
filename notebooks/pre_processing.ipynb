{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3cfaa0d",
   "metadata": {},
   "source": [
    "# Pre-processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df1682",
   "metadata": {},
   "source": [
    "## Imports e infos do dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10935d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d72bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/raw/bq-results-20250630-233421-1751326537000.csv\")\n",
    "\n",
    "# 1. Visão geral\n",
    "print(df.info())\n",
    "print(\"==\" * 40)\n",
    "\n",
    "# 2. Missing values\n",
    "print(\"Valores nulos: \", df.isna().sum())\n",
    "print(\"==\" * 40)\n",
    "print(\"Valores nulos p/col: \", df.isna().mean().sort_values(ascending=False))\n",
    "print(\"==\" * 40)\n",
    "\n",
    "# 3. Informações gerais\n",
    "print(\"Total de linhas:\", df.shape[0])\n",
    "print(\"Texto escrito no card:\", df[\"card_written_text\"].nunique())\n",
    "print(\"Texto falado do card:\", df[\"card_spoken_text\"].nunique())\n",
    "print(\"Localização do click:\", df[\"click_location\"].nunique())\n",
    "print(\"Quantidade de usuarios:\", df[\"user_uuid\"].nunique())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1755d4",
   "metadata": {},
   "source": [
    "## Limpar colunas principais nulas\n",
    "\n",
    "As principais colunas para o estudo são:\n",
    "\n",
    "- user_uuid (Identificador único dos usuários)\n",
    "- click_location (Localização do click na tela em coordenadas)\n",
    "- card_written_text (Texto escrito no cartão/pictograma)\n",
    "- event_timestamp (Registro de data e hora do click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7828a88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a lista de colunas que não podem ter valores nulos\n",
    "main_columns = ['user_uuid', 'click_location', 'card_written_text', 'event_timestamp']\n",
    "\n",
    "# Remove qualquer LINHA que tenha valor nulo em QUALQUER uma das colunas da lista acima\n",
    "df_clean = df.dropna(subset=main_columns).copy()\n",
    "\n",
    "print(\"Total de linhas:\", df.shape[0])\n",
    "print(\"Total de linhas após limpeza:\", df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428df392",
   "metadata": {},
   "source": [
    "## Criação de colunas com base no horário\n",
    "\n",
    "As principais colunas para o estudo são:\n",
    "\n",
    "- week_day (indica o dia da semana (Monday, Tuesday, Wednesday,\n",
    "Thursday, Friday, Saturday, Sunday))\n",
    "- hour (Representa as horas do dia (de 0 a 23))\n",
    "- year_num (Utilizado para a separação dos\n",
    "dados por número de semana.)\n",
    "- week_num (Representa o número da semana (1 a 52))\n",
    "- week_order (Calculada a partir das colunas\n",
    "week_num e year_num, atribuindo valores em ordem crescente)\n",
    "- period_day (O período do dia em que aconteceu a interação, sendo\n",
    "gerada a partir da coluna hour)\n",
    "\n",
    "### Horários por período:\n",
    "- midnight: (0-5h)\n",
    "- dawn: (6-8h)\n",
    "- morning: (9-11h)\n",
    "- noon: (12-14h)\n",
    "- afternoon: (15-17h)\n",
    "- evening: (18-20h)\n",
    "- night: (21-23h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação da coluna week_day\n",
    "df_clean[\"datetime\"] = pd.to_datetime(\n",
    "  df_clean[\"event_timestamp\"],\n",
    "  unit=\"us\",\n",
    "  errors=\"coerce\"\n",
    ")\n",
    "df_clean['week_day'] = df_clean['datetime'].dt.day_name()\n",
    "\n",
    "# Criação da coluna hour\n",
    "df_clean['hour'] = df_clean['datetime'].dt.hour\n",
    "\n",
    "# Criação da coluna year_num\n",
    "df_clean['year_num'] = df_clean['datetime'].dt.year\n",
    "\n",
    "# Criação da coluna week_num\n",
    "df_clean['week_num'] = df_clean['datetime'].dt.isocalendar().week\n",
    "\n",
    "# Criação da coluna week_order\n",
    "df_clean['week_order'] = df_clean['year_num'].astype(str) + '-' + df_clean['week_num'].astype(str)\n",
    "df_clean['week_order'] = pd.Categorical(df_clean['week_order'],\n",
    "                                  categories=df_clean['week_order'].unique(),\n",
    "                                  ordered=True)\n",
    "\n",
    "# Função para mapear hora → período\n",
    "def get_period(h):\n",
    "  if 6 <= h <= 8:\n",
    "    return 'dawn'\n",
    "  elif 9 <= h <= 11:\n",
    "    return 'morning'\n",
    "  elif 12 <= h <= 14:\n",
    "    return 'noon'\n",
    "  elif 15 <= h <= 17:\n",
    "    return 'afternoon'\n",
    "  elif 18 <= h <= 20:\n",
    "    return 'evening'\n",
    "  elif 21 <= h <= 23:\n",
    "    return 'night'\n",
    "  else:\n",
    "    return 'midnight'\n",
    "\n",
    "df_clean['period_day'] = df_clean['hour'].apply(get_period)\n",
    "\n",
    "# Ordenação customizada dos períodos\n",
    "period_order = ['midnight','dawn','morning','noon','afternoon','evening','night']\n",
    "df_clean['period_day'] = pd.Categorical(df_clean['period_day'], categories=period_order, ordered=True)\n",
    "\n",
    "\n",
    "df_clean = df_clean.drop(columns=['datetime'])\n",
    "# Exibir as primeiras linhas do DataFrame limpo\n",
    "# df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa47a42d",
   "metadata": {},
   "source": [
    "## Filtragem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ad19b5",
   "metadata": {},
   "source": [
    "### Filtragem dos usuários nos dados limpos\n",
    "\n",
    "De acordo com o estudo, foram feitas filtragem para treinar os modelos apenas com usuários que tiveram uma interação rica e diversificada com o aplicativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678b429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- ESTADO INICIAL ---\")\n",
    "print(f\"Total de linhas: {df_clean.shape[0]:,}\")\n",
    "print(f\"Total de usuários únicos: {df_clean['user_uuid'].nunique():,}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Cópia do DataFrame limpo para aplicar os filtros\n",
    "df_filtered = df_clean.copy()\n",
    "\n",
    "# --- Filtro 1: Mínimo de 50 cliques por usuário ---\n",
    "print(\"\\n>>> APLICANDO FILTRO 1: Mínimo de 50 cliques\")\n",
    "rows_before = df_filtered.shape[0]\n",
    "users_before = df_filtered['user_uuid'].nunique()\n",
    "\n",
    "user_clicks = df_filtered.groupby('user_uuid')['event_timestamp'].count()\n",
    "qualified_users = user_clicks[user_clicks >= 50].index\n",
    "df_filtered = df_filtered[df_filtered['user_uuid'].isin(qualified_users)]\n",
    "\n",
    "print(f\"   - Linhas removidas: {rows_before - df_filtered.shape[0]:,}\")\n",
    "print(f\"   - Usuários removidos: {users_before - df_filtered['user_uuid'].nunique():,}\")\n",
    "print(f\"   - Estado atual: {df_filtered.shape[0]:,} linhas e {df_filtered['user_uuid'].nunique():,} usuários.\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "\n",
    "# --- Filtro 2: Mínimo de 10 locais distintos de clique ---\n",
    "print(\"\\n>>> APLICANDO FILTRO 2: Mínimo de 10 locais distintos\")\n",
    "rows_before = df_filtered.shape[0]\n",
    "users_before = df_filtered['user_uuid'].nunique()\n",
    "\n",
    "user_locations = df_filtered.groupby('user_uuid')['click_location'].nunique()\n",
    "qualified_users = user_locations[user_locations >= 10].index\n",
    "df_filtered = df_filtered[df_filtered['user_uuid'].isin(qualified_users)]\n",
    "\n",
    "print(f\"   - Linhas removidas: {rows_before - df_filtered.shape[0]:,}\")\n",
    "print(f\"   - Usuários removidos: {users_before - df_filtered['user_uuid'].nunique():,}\")\n",
    "print(f\"   - Estado atual: {df_filtered.shape[0]:,} linhas e {df_filtered['user_uuid'].nunique():,} usuários.\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "\n",
    "# --- Filtro 3: Mínimo de 20 horários distintos de clique ---\n",
    "print(\"\\n>>> APLICANDO FILTRO 3: Mínimo de 20 horários distintos\")\n",
    "rows_before = df_filtered.shape[0]\n",
    "users_before = df_filtered['user_uuid'].nunique()\n",
    "\n",
    "user_hours = df_filtered.groupby('user_uuid')['hour'].nunique()\n",
    "qualified_users = user_hours[user_hours >= 20].index\n",
    "df_filtered = df_filtered[df_filtered['user_uuid'].isin(qualified_users)]\n",
    "\n",
    "print(f\"   - Linhas removidas: {rows_before - df_filtered.shape[0]:,}\")\n",
    "print(f\"   - Usuários removidos: {users_before - df_filtered['user_uuid'].nunique():,}\")\n",
    "print(f\"   - Estado atual: {df_filtered.shape[0]:,} linhas e {df_filtered['user_uuid'].nunique():,} usuários.\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "\n",
    "# --- Filtro 4: Mínimo de 1000 cliques por usuário ---\n",
    "print(\"\\n>>> APLICANDO FILTRO 4: Mínimo de 1000 cliques\")\n",
    "rows_before = df_filtered.shape[0]\n",
    "users_before = df_filtered['user_uuid'].nunique()\n",
    "\n",
    "user_clicks_1000 = df_filtered.groupby('user_uuid')['event_timestamp'].count()\n",
    "qualified_users = user_clicks_1000[user_clicks_1000 >= 1000].index\n",
    "df_filtered = df_filtered[df_filtered['user_uuid'].isin(qualified_users)]\n",
    "\n",
    "print(f\"   - Linhas removidas: {rows_before - df_filtered.shape[0]:,}\")\n",
    "print(f\"   - Usuários removidos: {users_before - df_filtered['user_uuid'].nunique():,}\")\n",
    "print(f\"   - Estado atual: {df_filtered.shape[0]:,} linhas e {df_filtered['user_uuid'].nunique():,} usuários.\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "\n",
    "# --- Filtro 5: Mínimo de 3 semanas de utilização ---\n",
    "print(\"\\n>>> APLICANDO FILTRO 5: Mínimo de 3 semanas de utilização\")\n",
    "rows_before = df_filtered.shape[0]\n",
    "users_before = df_filtered['user_uuid'].nunique()\n",
    "\n",
    "user_weeks = df_filtered.groupby('user_uuid')['week_order'].nunique()\n",
    "qualified_users = user_weeks[user_weeks >= 3].index\n",
    "df_filtered = df_filtered[df_filtered['user_uuid'].isin(qualified_users)]\n",
    "\n",
    "print(f\"   - Linhas removidas: {rows_before - df_filtered.shape[0]:,}\")\n",
    "print(f\"   - Usuários removidos: {users_before - df_filtered['user_uuid'].nunique():,}\")\n",
    "print(f\"   - Estado atual: {df_filtered.shape[0]:,} linhas e {df_filtered['user_uuid'].nunique():,} usuários.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "\n",
    "# --- RESULTADO FINAL ---\n",
    "print(\"\\n--- ESTADO FINAL ---\")\n",
    "print(f\"Total de linhas no dataset final: {df_filtered.shape[0]:,}\")\n",
    "print(f\"Total de usuários únicos no dataset final: {df_filtered['user_uuid'].nunique():,}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ae12a6",
   "metadata": {},
   "source": [
    "### Converter texto do pictograma pra minúsculo e mantendo apenas últimos cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c24dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo o texto escrito no card para minúsculas\n",
    "df_filtered['card_written_text'] = df_filtered['card_written_text'].apply(lambda s: s.lower() if isinstance(s, str) else s)\n",
    "\n",
    "# Filtrando apenas por 'card_is_leaf' = True\n",
    "df_filtered = df_filtered[df_filtered['card_is_leaf'] == True].copy()\n",
    "print(\"\\n--- Filtro aplicado: Apenas cards finais ---\")\n",
    "print(f\"Total de linhas após filtro: {df_filtered.shape[0]:,}\")\n",
    "print(f\"Total de usuários únicos no dataset final: {df_filtered['user_uuid'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e66631",
   "metadata": {},
   "source": [
    "### Filtrando as colunas importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea689b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['user_uuid', 'click_location', 'card_written_text', 'event_timestamp', 'week_day', 'hour', 'year_num', 'week_num', 'week_order', 'period_day']\n",
    "df_filtered = df_filtered.loc[:, columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ac9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o DataFrame final filtrado\n",
    "path_parquet = '../data/processed/df_filtered.parquet'\n",
    "df_filtered.to_parquet(path_parquet, index=False)\n",
    "\n",
    "print(f\"DataFrame filtrado salvo com sucesso em: {path_parquet}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e23d7",
   "metadata": {},
   "source": [
    "## Divisão de Treino e Teste e clusterização do conjunto de Treino\n",
    "\n",
    "A coluna 'cluster' foi criada a partir da clusterização com ajuda do algoritmo OPTICS, um método baseado em densidade que ordena os pontos de um conjunto de dados para revelar estruturas de agrupamento. Essa coluna será utilizada como contexto no treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c6f01a",
   "metadata": {},
   "source": [
    "### 1. Preparação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a06e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar as coordenadas\n",
    "location_coords = df_filtered['click_location'].astype(str).str.split(',', expand=True)\n",
    "\n",
    "# Criar as novas colunas numéricas no df_filtered\n",
    "df_filtered['click_loc_x'] = pd.to_numeric(location_coords[0], errors='coerce')\n",
    "df_filtered['click_loc_y'] = pd.to_numeric(location_coords[1], errors='coerce')\n",
    "\n",
    "# Preencher valores nulos com a mediana\n",
    "df_filtered['click_loc_x'] = df_filtered['click_loc_x'].fillna(df_filtered['click_loc_x'].median())\n",
    "df_filtered['click_loc_y'] = df_filtered['click_loc_y'].fillna(df_filtered['click_loc_y'].median())\n",
    "\n",
    "print(\"Engenharia de features concluída. Novas colunas adicionadas ao df_filtered:\")\n",
    "print(df_filtered[['click_loc_x', 'click_loc_y', 'hour']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c754ca39",
   "metadata": {},
   "source": [
    "### 2. Função para geração dos clusters no Treino e porpagação no Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daa1d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gerar_features_cluster(train_df, test_df, feature_cols, optics_params):\n",
    "  \"\"\"\n",
    "  Aplica a clusterização no conjunto de treino e propaga os rótulos para o conjunto de teste.\n",
    "\n",
    "  Args:\n",
    "      train_df (pd.DataFrame): DataFrame de treino do usuário.\n",
    "      test_df (pd.DataFrame): DataFrame de teste do usuário.\n",
    "      feature_cols (list): Lista de nomes de colunas a serem usadas para a clusterização.\n",
    "      optics_params (dict): Dicionário com os parâmetros para o algoritmo OPTICS.\n",
    "\n",
    "  Returns:\n",
    "      tuple: Uma tupla contendo (train_df_com_cluster, test_df_com_cluster).\n",
    "  \"\"\"\n",
    "  print(f\"   - Iniciando clusterização para {len(train_df)} registros de treino...\")\n",
    "\n",
    "  # Copia os DataFrames para evitar warnings de cópia\n",
    "  train_df_out = train_df.copy()\n",
    "  test_df_out = test_df.copy()\n",
    "\n",
    "  # --- Etapa 1: Clusterizar o Conjunto de Treino ---\n",
    "  features_train = train_df_out[feature_cols]\n",
    "\n",
    "  # O scaler é treinado APENAS nos dados de treino\n",
    "  scaler = StandardScaler()\n",
    "  features_train_scaled = scaler.fit_transform(features_train)\n",
    "\n",
    "  # O OPTICS é executado APENAS nos dados de treino\n",
    "  optics = OPTICS(**optics_params)\n",
    "  train_clusters = optics.fit_predict(features_train_scaled)\n",
    "  train_df_out['cluster'] = train_clusters\n",
    "\n",
    "  print(f\"   - Finalizado. Encontrados {len(pd.Series(train_clusters).unique())} clusters.\")\n",
    "\n",
    "\n",
    "  # --- Etapa 2: Propagar os Rótulos para o Conjunto de Teste ---\n",
    "  # O KNN aprende a mapear as features do treino para os clusters do treino\n",
    "  knn = KNeighborsClassifier(n_neighbors=5)\n",
    "  knn.fit(features_train_scaled, train_clusters)\n",
    "\n",
    "  # Usa o MESMO scaler para transformar os dados de teste\n",
    "  features_test = test_df_out[feature_cols]\n",
    "  features_test_scaled = scaler.transform(features_test)\n",
    "\n",
    "  # Prevê os clusters para os dados de teste\n",
    "  test_clusters = knn.predict(features_test_scaled)\n",
    "  test_df_out['cluster'] = test_clusters\n",
    "\n",
    "  return train_df_out, test_df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32bc2ea",
   "metadata": {},
   "source": [
    "### 3. Preparando os encoders globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806a0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Preparando Encoder Global de Usuários ---\")\n",
    "global_encoder_path = '../models/global_encoders'\n",
    "os.makedirs(global_encoder_path, exist_ok=True)\n",
    "\n",
    "# O único encoder global agora é o de usuários\n",
    "all_user_uuids = df_filtered[\"user_uuid\"].unique()\n",
    "le_user = LabelEncoder().fit(all_user_uuids)\n",
    "joblib.dump(le_user, os.path.join(global_encoder_path, \"label_encoder_user.pkl\"))\n",
    "print(f\"LabelEncoder de usuários treinado e salvo. Total de usuários: {len(le_user.classes_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5447ca8d",
   "metadata": {},
   "source": [
    "### 4. Divisão, clusterização e encoding por usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e024be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros usados para a clusterização\n",
    "FEATURES_CLUSTER = ['click_loc_x', 'click_loc_y', 'hour']\n",
    "\n",
    "# Listas para guardar os dataframes processados de cada usuário\n",
    "train_parts = []\n",
    "test_parts = []\n",
    "\n",
    "# Lista para DataFrame que será utilizado para visualização dos clusters\n",
    "train_parts_for_viz = []\n",
    "\n",
    "# Ordena o DataFrame para garantir que a divisão por semanas seja consistente\n",
    "df_filtered_sorted = df_filtered.sort_values(['user_uuid', 'event_timestamp'])\n",
    "lista_usuarios = df_filtered_sorted['user_uuid'].unique()\n",
    "\n",
    "for i, user_id in enumerate(lista_usuarios):\n",
    "    print(f\"\\n--- Processando user_{i} ---\")\n",
    "    user_model_path = f'../models/user_{i}'\n",
    "    os.makedirs(user_model_path, exist_ok=True)\n",
    "\n",
    "    # DataFrame do usuário\n",
    "    df_u = df_filtered_sorted[df_filtered_sorted['user_uuid'] == user_id]\n",
    "\n",
    "    # Treinando o LabelEncoder de Cartões antes da divisão, para garantir que ele seja treinado com todos os dados do usuário\n",
    "    le_card_user = LabelEncoder().fit(df_u[\"card_written_text\"])\n",
    "    joblib.dump(le_card_user, os.path.join(user_model_path, \"label_encoder_card.pkl\"))\n",
    "    print(f\"   - Label Encoder de cards treinado e salvo em {user_model_path}\")\n",
    "\n",
    "    # Divisão Treino/Teste\n",
    "    semanas = sorted(df_u['week_order'].unique())\n",
    "    if len(semanas) < 3:\n",
    "        continue\n",
    "    df_u = df_u.sort_values('week_order')\n",
    "    weeks_test, weeks_train = semanas[-2:], semanas[:-2]\n",
    "    df_train, df_test = df_u[df_u['week_order'].isin(weeks_train)].copy(), df_u[df_u['week_order'].isin(weeks_test)].copy()\n",
    "\n",
    "    if df_train.empty or df_test.empty:\n",
    "        continue\n",
    "\n",
    "    # Define min_samples como 3.5% dos dados de treino, com um mínimo de 50\n",
    "    dynamic_min_samples = max(50, int(len(df_train) * 0.035))\n",
    "    # Define o min_cluster_size como o dobro, para filtrar clusters menores\n",
    "    dynamic_min_cluster_size = dynamic_min_samples * 2\n",
    "\n",
    "    params_optics_user = {\n",
    "        'min_samples': dynamic_min_samples,\n",
    "        'xi': 0.05,\n",
    "        'min_cluster_size': dynamic_min_cluster_size\n",
    "    }\n",
    "    print(f\"   - Usando parâmetros OPTICS dinâmicos: {params_optics_user}\")\n",
    "\n",
    "    # Clusterização\n",
    "    df_train, df_test = gerar_features_cluster(df_train, df_test, FEATURES_CLUSTER, params_optics_user)\n",
    "\n",
    "    # Salvando o DataFrame para visualização dos clusters\n",
    "    df_train['user_uuid_enc'] = le_user.transform(df_train[\"user_uuid\"])\n",
    "    train_parts_for_viz.append(df_train)\n",
    "\n",
    "    print(\"   - Aplicando Encoders...\")\n",
    "    # Label Encoder de Cartões (aplicado aos dataframes de treino e teste)\n",
    "    df_train['card_enc'] = le_card_user.transform(df_train[\"card_written_text\"])\n",
    "    df_test['card_enc'] = le_card_user.transform(df_test[\"card_written_text\"])\n",
    "\n",
    "    # Label Encoder de Usuário (usando o encoder GLOBAL)\n",
    "    df_train['user_uuid_enc'] = le_user.transform(df_train[\"user_uuid\"])\n",
    "    df_test['user_uuid_enc'] = le_user.transform(df_test[\"user_uuid\"])\n",
    "\n",
    "    # One-Hot Encoding (treinado por usuário)\n",
    "    cols_ctx = ['period_day', 'week_day', 'cluster']\n",
    "    ohe_ctx = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "    # Treina o OHE com os dados de treino do usuário e o salva\n",
    "    ohe_ctx.fit(df_train[cols_ctx])\n",
    "    joblib.dump(ohe_ctx, os.path.join(user_model_path, \"onehot_encoder.pkl\"))\n",
    "    print(f\"   - One-Hot Encoder treinado e salvo em {user_model_path}\")\n",
    "\n",
    "    # Transforma ambos os dataframes e cria as novas colunas\n",
    "    for df_part, name in [(df_train, 'train'), (df_test, 'test')]:\n",
    "        ctx_encoded = ohe_ctx.transform(df_part[cols_ctx])\n",
    "        feature_names = ohe_ctx.get_feature_names_out(cols_ctx)\n",
    "\n",
    "        df_encoded = pd.DataFrame(ctx_encoded, columns=feature_names, index=df_part.index)\n",
    "\n",
    "        # Remove colunas antigas e junta com as novas\n",
    "        df_part = df_part.drop(columns=[\"card_written_text\", \"user_uuid\"] + cols_ctx)\n",
    "        df_final_user = pd.concat([df_part, df_encoded], axis=1)\n",
    "\n",
    "        # Salva o resultado final do usuário\n",
    "        output_path = os.path.join(user_model_path, f\"{name}_processed.parquet\")\n",
    "        df_final_user.to_parquet(output_path, compression=\"snappy\")\n",
    "        print(f\"   - Arquivo salvo: {output_path}\")\n",
    "\n",
    "        # Adiciona o DataFrame final do usuário à lista de partes\n",
    "        if name == 'train':\n",
    "            train_parts.append(df_final_user)\n",
    "        else:\n",
    "            test_parts.append(df_final_user)\n",
    "\n",
    "print(\"\\n--- Juntando e salvando os resultados finais de todos os usuários ---\")\n",
    "df_train_final = pd.concat(train_parts, ignore_index=True)\n",
    "df_test_final = pd.concat(test_parts, ignore_index=True)\n",
    "df_for_viz = pd.concat(train_parts_for_viz, ignore_index=True)\n",
    "\n",
    "# Salva os arquivos finais\n",
    "output_path_train = '../data/processed/train_final_processed.parquet'\n",
    "output_path_test = '../data/processed/test_final_processed.parquet'\n",
    "output_path_viz = '../data/processed/data_for_visualization.parquet'\n",
    "df_train_final.to_parquet(output_path_train, index=False, compression=\"snappy\")\n",
    "df_test_final.to_parquet(output_path_test, index=False, compression=\"snappy\")\n",
    "df_for_viz.to_parquet(output_path_viz, index=False, compression=\"snappy\")\n",
    "print(f\"   - Arquivo de treino final salvo em: {output_path_train}\")\n",
    "print(f\"   - Arquivo de teste final salvo em: {output_path_test}\")\n",
    "print(f\"   - Arquivo para visualização salvo em: {output_path_viz}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Processamento Finalizado para Todos os Usuários ---\")\n",
    "print(f\"Tamanho final do treino (todos os usuários): {len(df_train_final)} linhas\")\n",
    "print(f\"Tamanho final do teste (todos os usuários): {len(df_test_final)} linhas\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
